# はじめに

AI システムのセキュリティ テストは、以下の 3 つの戦略に基づいています。

- **従来のセキュリティ テスト**（*ペネトレーション テスト*など）。[セキュア ソフトウェア開発](https://owaspai.org/goto/secdevprogram/)を参照してください。
- **モデルのパフォーマンス検証**（[継続的な検証](https://owaspai.org/goto/continuousvalidation/)を参照）: モデルの意図された動作を代表する入力と出力を含む検証セットを使用して、モデルが指定された受け入れ基準に従って動作するかどうかをテストします。セキュリティに関しては、データ汚染またはモデル汚染によってモデルの動作が恒久的に変更されていないかどうかを検出します。セキュリティ以外の分野では、機能の正確性、モデルのドリフトなどをテストします。
- **AI セキュリティテスト**（本セクション）: AI レッド チーム演習の一部であり、特定の攻撃をシミュレートすることで、AI モデルが特定の攻撃に耐えられるかどうかをテストします。

AI セキュリティ テストでは、敵対的な行動をシミュレートして、AI システムの脆弱性、弱点、リスクを明らかにします。従来の AI テストの焦点は機能とパフォーマンスですが、AI レッド チーム演習の焦点は標準的な検証を超え、意図的なストレス テスト、攻撃、安全対策の回避の試みなどが含まれます。レッド チームの焦点はセキュリティだけにとどまりませんが、本稿では主に「AI セキュリティのための AI レッド チーム」に焦点を当てます。

本セクションでは、予測 AI と生成 AI のそれぞれの性質、リスク、および適用方法が異なるため、AI レッド チームを区別します。開発時のサプライチェーンにおける脅威など、一部の脅威は両方の AI に共通する場合もありますが、それぞれの適用場面で顕在化する方法は大きく異なる場合があります。

AI レッド チーム演習への体系的なアプローチには、以下の重要なステップが含まれます。

- **目標と範囲の定義**: 目標の特定、組織、コンプライアンス、およびリスク管理要件との整合。
- **AI システムの理解**: モデル、使用事例、導入シナリオの詳細。
- **潜在的な脅威の特定**: 脅威モデリング、攻撃対象領域の特定、調査、脅威アクター。
- **攻撃シナリオの策定**: 攻撃シナリオとエッジ ケースの設計。
- **テスト実行**: 攻撃シナリオに対する手動または自動テストの実施。
- **リスク評価**: 特定された脆弱性とリスクの文書化。
- **優先順位付けとリスク軽減**: 修復のための行動計画の策定、軽減策の実施、残留リスクの算出。
- **修正の検証**: 修復後のシステムの再テスト。

AI セキュリティテストの詳細については、[OWASP AI Testing Guide](https://github.com/OWASP/www-project-ai-testing-guide) を参照してください。

# テストの対象となる脅威

資産、影響、攻撃対象領域に基づいた包括的な脅威と管理策のリストは、[AI セキュリティ基本要素表](https://owaspai.org/goto/periodictable/)として利用可能となっています。このセクションでは、AI レッド チーム演習（予測 AI および生成 AI システム）用のツールのリストを提供し、攻撃シナリオの作成、自動レッドチーム演習によるテスト実行、そして多くの場合、リスクスコアリングによるリスク評価といったステップを支援します。

リストされている各ツールは、AI システムの脅威情勢のサブセットに対応しています。以下に、考慮すべき主要な脅威をいくつか挙げます。

## 予測 AI

予測 AI システムは、入力データに基づいて予測または分類を行うように設計されています。例としては、不正検出、画像認識、レコメンデーション システムなどが挙げられます。

### 予測 AI に対する主要な脅威

- [回避攻撃](https://owaspai.org/goto/evasion/): これらの攻撃は、攻撃者がモデルを誤誘導する入力を作成し、タスクを誤って実行させる場合に発生します。
- [モデル盗難](https://owaspai.org/goto/modeltheftuse/): この攻撃では、モデルのパラメータまたは機能が盗まれます。これにより、攻撃者は複製モデルを作成し、それを敵対的攻撃やその他の複合的な脅威を作成するためのオラクルとして利用できるようになります。
- [モデル汚染](https://owaspai.org/goto/modelpoison/): これは、学習フェーズ（開発フェーズ）中にデータ、データパイプライン、またはモデル学習のサプライチェーンを操作することです。攻撃者の目的は、モデルの動作を変更し、望ましくないモデルの動作を引き起こすことです。

## 生成 AI

生成 AI システムは、テキスト、画像、音声などの出力を生成します。例としては、ChatGPT のような大規模言語モデル（LLM）や、DALL-E や MidJourney のような大規模視覚モデル（LVM）が挙げられます。

### 生成 AI に対する主要な脅威

- [プロンプト インジェクション](https://owaspai.org/goto/promptinjection/): この種類の攻撃では、攻撃者はモデルに不正な指示を与え、悪意のある結果や目的を達成しようとします。
- [実行時の直接的なモデル盗難](https://owaspai.org/goto/runtimemodeltheft/): 攻撃者はモデルの一部、またはシステムプロンプトなどの重要なコンポーネントを標的とします。これにより、ガードレールを回避する高度な入力を作成できるようになります。
- [セキュアでない出力処理](https://owaspai.org/goto/insecureoutput/): 生成 AI システムは、従来のインジェクション攻撃に対して脆弱である可能性があり、出力が適切に処理されないとリスクにつながります。

各 AI パラダイムにおける主要な脅威について説明しましたが、AI レッド チーム演習の目的と範囲の定義フェーズの結果に基づき、AI Exchange ですべての脅威を参照することを強くお勧めします。

## AI セキュリティ テストに関する参考資料

- エージェント型 AI システムのテストの詳細については、CSA と AI Exchange が共同で作成した [Agentic AI red teaming guide](https://cloudsecurityalliance.org/download/artifacts/agentic-ai-red-teaming-guide) を参照してください。
- [OWASP AI Testing Guide](https://github.com/OWASP/www-project-ai-testing-guide)