# はじめに

AI システムのセキュリティ テストは、以下の 3 つの戦略に基づいています。

- **従来のセキュリティ テスト**（*ペネトレーション テスト*など）。[セキュア ソフトウェア開発](https://owaspai.org/goto/secdevprogram/)を参照してください。
- **モデルのパフォーマンス検証**（[継続的な検証](https://owaspai.org/goto/continuousvalidation/)を参照）: モデルの意図された動作を代表する入力と出力を含む検証セットを使用して、モデルが指定された受け入れ基準に従って動作するかどうかをテストします。セキュリティに関しては、データ汚染またはモデル汚染によってモデルの動作が恒久的に変更されていないかどうかを検出します。セキュリティ以外の分野では、機能の正確性、モデルのドリフトなどをテストします。
- **AI セキュリティ テスト**（本セクション）: AI レッド チーム演習の一部であり、特定の攻撃をシミュレートすることで、AI モデルが特定の攻撃に耐えられるかどうかをテストします。

AI セキュリティ テストでは、敵対的な行動をシミュレートして、AI システムの脆弱性、弱点、リスクを明らかにします。従来の AI テストの焦点は機能とパフォーマンスですが、AI レッド チーム演習の焦点は標準的な検証を超え、意図的なストレス テスト、攻撃、安全対策の回避の試みなどが含まれます。レッド チームの焦点はセキュリティだけにとどまりませんが、本稿では主に「AI セキュリティのための AI レッド チーム」に焦点を当てます。

本セクションでは、予測 AI と生成 AI のそれぞれの性質、リスク、および適用方法が異なるため、AI レッド チームを区別します。開発時のサプライチェーンにおける脅威など、一部の脅威は両方の AI に共通する場合もありますが、それぞれの適用場面で顕在化する方法は大きく異なる場合があります。

AI レッド チーム演習への体系的なアプローチには、以下の重要なステップが含まれます。

- **目標と範囲の定義**: 目標の特定、組織、コンプライアンス、およびリスク管理要件との整合。
- **AI システムの理解**: モデル、使用事例、導入シナリオの詳細。
- **潜在的な脅威の特定**: 脅威モデリング、攻撃対象領域の特定、調査、脅威アクター。
- **攻撃シナリオの策定**: 攻撃シナリオとエッジ ケースの設計。
- **テスト実行**: 攻撃シナリオに対する手動または自動テストの実施。
- **リスク評価**: 特定された脆弱性とリスクの文書化。
- **優先順位付けとリスク軽減**: 修復のための行動計画の策定、軽減策の実施、残留リスクの算出。
- **修正の検証**: 修復後のシステムの再テスト。

AI セキュリティテストの詳細については、[OWASP AI Testing Guide](https://github.com/OWASP/www-project-ai-testing-guide) を参照してください。

# テストの対象となる脅威

資産、影響、攻撃対象領域に基づいた包括的な脅威と管理策のリストは、[AI セキュリティ基本要素表](https://owaspai.org/goto/periodictable/)として利用可能となっています。このセクションでは、AI レッド チーム演習（予測 AI および生成 AI システム）用のツールのリストを提供し、攻撃シナリオの作成、自動レッドチーム演習によるテスト実行、そして多くの場合、リスクスコアリングによるリスク評価といったステップを支援します。

リストされている各ツールは、AI システムの脅威情勢のサブセットに対応しています。以下に、考慮すべき主要な脅威をいくつか挙げます。

## 予測 AI

予測 AI システムは、入力データに基づいて予測または分類を行うように設計されています。例としては、不正検出、画像認識、レコメンデーション システムなどが挙げられます。

### 予測 AI に対する主要な脅威

- [回避攻撃](https://owaspai.org/goto/evasion/): これらの攻撃は、攻撃者がモデルを誤誘導する入力を作成し、タスクを誤って実行させる場合に発生します。
- [モデル盗難](https://owaspai.org/goto/modeltheftuse/): この攻撃では、モデルのパラメータまたは機能が盗まれます。これにより、攻撃者は複製モデルを作成し、それを敵対的攻撃やその他の複合的な脅威を作成するためのオラクルとして利用できるようになります。
- [モデル汚染](https://owaspai.org/goto/modelpoison/): これは、学習フェーズ（開発フェーズ）中にデータ、データパイプライン、またはモデル学習のサプライチェーンを操作することです。攻撃者の目的は、モデルの動作を変更し、望ましくないモデルの動作を引き起こすことです。

## 生成 AI

生成 AI システムは、テキスト、画像、音声などの出力を生成します。例としては、ChatGPT のような大規模言語モデル（LLM）や、DALL-E や MidJourney のような大規模視覚モデル（LVM）が挙げられます。

### 生成 AI に対する主要な脅威

- [プロンプト インジェクション](https://owaspai.org/goto/promptinjection/): この種類の攻撃では、攻撃者はモデルに不正な指示を与え、悪意のある結果や目的を達成しようとします。
- [実行時の直接的なモデル盗難](https://owaspai.org/goto/runtimemodeltheft/): 攻撃者はモデルの一部、またはシステムプロンプトなどの重要なコンポーネントを標的とします。これにより、ガードレールを回避する高度な入力を作成できるようになります。
- [セキュアでない出力処理](https://owaspai.org/goto/insecureoutput/): 生成 AI システムは、従来のインジェクション攻撃に対して脆弱である可能性があり、出力が適切に処理されないとリスクにつながります。

各 AI パラダイムにおける主要な脅威について説明しましたが、AI レッド チーム演習の目的と範囲の定義フェーズの結果に基づき、AI Exchange ですべての脅威を参照することを強くお勧めします。

## AI セキュリティ テストに関する参考情報

- エージェント型 AI システムのテストの詳細については、CSA と AI Exchange が共同で作成した [Agentic AI red teaming guide](https://cloudsecurityalliance.org/download/artifacts/agentic-ai-red-teaming-guide) を参照してください。
- [OWASP AI Testing Guide](https://github.com/OWASP/www-project-ai-testing-guide)

# AI および 生成 AI 向けのレッド  チーム演習ツール

以下のマインド マップは、AI レッド チーム演習のためのオープン ソース ツールの概要を示しています。予測 AI 向けのレッド チーム演習と生成 AI 向けのレッド チーム演習の 2 つに分類し、ART、Armory、TextAttack、Promptfoo などの例を取り上げています。これらのツールは現時点で利用可能な機能を示したものですが、網羅的ではなく、重要度順にランク付けされているわけでもありません。今後、新たなツールや手法が登場し、この分野に統合される可能性があります。

![attackstotesttools.png](./assets/attackstotesttools.png)

以下の図は、AI システムにおける脅威を分類し、それらの脅威に対処するために設計された関連するオープン ソース  ツールにマッピングしています。

![testtoolstoattacks.png](./assets/testtoolstoattacks.png)

以下のセクションでは、予測 AI 用のツールについて説明し、その後に生成 AI 用のセクションに進みます。

## 予測 AI レッド チーム演習のためのオープン ソース ツール

このサブセクションでは、予測 AI のセキュリティ テストに使用できるツール Adversarial Robustness Toolbox (ART)、Armory、Foolbox、DeepSec、TextAttack について説明します。

### ツール名: Adversarial Robustness Toolbox (ART)

<div align="center">
<table>
<tr>
	<th width="30%">要素</th>
	<th width="70%">詳細</th>
</tr>
<tr>
	<td>開発者/ソース</td>
	<td>IBM Research / the Linux Foundation AI & Data Foundation (LF AI & Data)</td>
</tr>
<tr>
	<td>GitHub リンク</td>
	<td><a href="https://github.com/Trusted-AI/adversarial-robustness-toolbox">https://github.com/Trusted-AI/adversarial-robustness-toolbox</a></td>
</tr>
<tr>
	<td>言語</td>
	<td>Python</td>
</tr>
<tr>
	<td>ライセンス</td>
	<td>MIT ライセンス下のオープン ソース</td>
</tr>
<tr>
	<td>軽減策の提供</td>
	<td>防止: なし、検出: あり</td>
</tr>
<tr>
	<td>API の利用</td>
	<td>可能</td>
</tr>
<tr>
	<td>人気</td>
	<td><ul><li>GitHub スター: 約 4,900 スター （2024 年現在）<li>GitHub フォーク: 約 1,200 フォーク<li>Issue 数: 約 131 件の未解決、761 件の解決済み<li>トレンド: 敵対的堅牢性のための継続的なアップデートと業界での採用により、着実に成長しています。</ul></td>
</tr>
<tr>
	<td>コミュニティ サポート</td>
	<td><ul><li>アクティブな問題: 対応力の高いチームで、通常は 1 週間以内に問題に対処します。<li>文書化: IBM の Web サイトに包括的なガイドと API 文書が用意されており、詳細かつ定期的に更新されます。<li>ディスカッション フォーラム: 主に学術的な環境で議論され、Stack Overflow や GitHub でも一部議論されています。<li>貢献者: IBM の研究者や外部の協力者を含む 100 人を超えます。</ul></td>
</tr>
<tr>
	<td>スケーラビリティ</td>
	<td><ul><li>フレームワーク サポート:すぐに使用できるサポートにより、TensorFlow、Keras、PyTorch に拡張できます。<li>大規模なデプロイメント: 医療、金融、防衛などの業界における大規模なエンタープライズ レベルのデプロイメントに対応できることが実証されています。</ul></td>
</tr>
<tr>
	<td>統合</td>
	<td><ul><li>互換性: TensorFlow、PyTorch、Keras、MXNet、Scikit-learn で動作します。</ul></td>
</tr>
</table>
</div>

#### ツールの評価

| 基準 | 評価 |
| ----- | ----- |
| 人気 | 高 |
| コミュニティ サポート | 高 |
| スケーラビリティ | 高 |
| 統合の容易さ | 高 |

#### データのモダリティ

| データのモダリティ | サポート有無 |
| ----- | ----- |
| 文章 | ✅ |
| 画像 | ✅ |
| 音声 | ✅ |
| 映像 | ✅ |
| 表形式のデータ | ✅ |

#### 機械学習タスク

| タスクの種類 | データのモダリティ | サポート有無 |
| ----- | ----- | ----- |
| 分類 | すべて（「データのモダリティ」セクションを参照） | ✅ |
| 物体検出 | コンピューター ビジョン | ✅ |
| 音声認識 | 音声 | ✅ |

#### フレームワークの適用性

| フレームワーク/ツール | カテゴリ | サポート有無 |
| ----- | ----- | ----- |
| Tensorflow | 深層学習、生成 AI | ✅ |
| Keras	| 深層学習、生成 AI | ✅ |
| PyTorch	| 深層学習、生成 AI | ✅ |
| MxNet	| 深層学習 | ✅ |
| Scikit-learn | 機械学習 | ✅ |
| XGBoost | 機械学習 | ✅ |
| LightGBM | 機械学習 | ✅ |
| CatBoost | 機械学習 | ✅ |
| GPy | 機械学習 | ✅ |

#### OWASP AI Exchange の脅威の網羅性

| 脅威 | 対応有無 |
| ----- | ----- |
| 開発時のモデル汚染 | ✅ |
| 実行時のモデル汚染 |  |
| 利用時のモデル盗難 | ✅ |
| 学習データの汚染 |  |
| 学習データの漏洩 |  |
| 実行時のモデル盗難 |  |
| 回避（敵対的な入力に対するモデルのパフォーマンスのテスト）| ✅ |
| モデル反転/メンバーシップ推論 | ✅ |
| モデルのサービス運用妨害 |  |
| 直接プロンプト インジェクション |  |
| データの開示 |  |
| モデル入力の漏洩 |  |
| 間接プロンプト インジェクション |  |
| 開発時のモデル盗難 |  |
| インジェクション攻撃を含む出力 |  |

注:

- [開発時のモデル汚染](https://owaspai.org/goto/modelpoison/): 開発中に攻撃をシミュレートして脆弱性を評価します。
- [回避](https://owaspai.org/goto/evasion/): 敵対的な入力に対するモデルのパフォーマンスをテストします。
- [利用時のモデル盗難](https://owaspai.org/goto/modeltheftuse): 利用中のモデル悪用リスクを評価します。
- [モデル推論](https://owaspai.org/goto/modelinversionandmembership/): *メンバーシップ攻撃と反転攻撃への露出を評価します*。

### ツール名: Armory

<div align="center">
<table>
<tr>
	<th width="30%">要素</th>
	<th width="70%">詳細</th>
</tr>
<tr>
	<td>開発者/ソース</td>
	<td>MITRE Corporation</td>
</tr>
<tr>
	<td>GitHub リンク</td>
	<td><a href="https://github.com/twosixlabs/armory-libraryhttps://github.com/twosixlabs/armory">https://github.com/twosixlabs/armory-libraryhttps://github.com/twosixlabs/armory</a></td>
</tr>
<tr>
	<td>言語</td>
	<td>Python</td>
</tr>
<tr>
	<td>ライセンス</td>
	<td>MIT ライセンス下のオープン ソース</td>
</tr>
<tr>
	<td>軽減策の提供</td>
	<td>防止: なし、検出: あり</td>
</tr>
<tr>
	<td>API の利用</td>
	<td>可能</td>
</tr>
<tr>
	<td>人気</td>
	<td><ul><li>GitHub スター: 約 176 スター （2024 年現在）<li>GitHub フォーク: 約 67 フォーク<li>Issue 数: 約 59 件の未解決、733 件の解決済み、26 人の貢献者<li>トレンド: 特に防衛およびサイバーセキュリティ分野で成長しています。</ul></td>
</tr>
<tr>
	<td>コミュニティ サポート</td>
	<td><ul><li>アクティブな問題: 問題に対する迅速な対応（通常は数日から 1 週間以内に解決されます）。<li>文書化: 包括的ですが、セキュリティに重点を置いており、敵対的な攻撃と防御に関する高度なチュートリアルが含まれています。<li>ディスカッション フォーラム: GitHub での活発なディスカッション、セキュリティ固有のフォーラム（例: DARPA プロジェクト関連）への参加。<li>貢献者: 40 人以上の貢献者（ほとんどがセキュリティの専門家と研究者）</ul></td>
</tr>
<tr>
	<td>スケーラビリティ</td>
	<td><ul><li>フレームワーク サポート: TensorFlow と Keras をネイティブにサポートし、PyTorch の統合オプションもいくつか用意されています。<li>大規模なデプロイメント: 主にセキュリティ関連のデプロイメントで使用されます。セキュリティ以外のタスクのスケーラビリティについてはあまり文書化されていません。</ul></td>
</tr>
<tr>
	<td>統合</td>
	<td><ul><li>互換性:  TensorFlow および Keras と連携可能。IBM ART との統合により堅牢性が向上します。<li>API の可用性: IBM ART と比較すると制限がありますが、敵対的 ML の使用事例には十分です。</ul></td>
</tr>
</table>
</div>

#### ツールの評価

| 基準 | 評価 |
| ----- | ----- |
| 人気 | 低 |
| コミュニティ サポート | 中 |
| スケーラビリティ | 中 |
| 統合の容易さ | 高 |

#### データのモダリティ

| データのモダリティ | サポート有無 |
| ----- | ----- |
| 文章 | ✅ |
| 画像 | ✅ |
| 音声 | ✅ |
| 映像 | ✅ |
| 表形式のデータ | ✅ |

#### 機械学習タスク

| タスクの種類 | データのモダリティ | サポート有無 |
| ----- | ----- | ----- |
| 分類 | すべて（「データのモダリティ」セクションを参照） | ✅ |
| 物体検出 | コンピューター ビジョン | ✅ |
| 音声認識 | 音声 | ✅ |

#### フレームワークの適用性

| フレームワーク/ツール | カテゴリ | サポート有無 |
| ----- | ----- | ----- |
| Tensorflow | 深層学習、生成 AI | ✅ |
| Keras	| 深層学習、生成 AI |  |
| PyTorch	| 深層学習、生成 AI | ✅ |
| MxNet	| 深層学習 |  |
| Scikit-learn | 機械学習 |  |
| XGBoost | 機械学習 |  |
| LightGBM | 機械学習 |  |
| CatBoost | 機械学習 |  |
| GPy | 機械学習 |  |

#### OWASP AI Exchange の脅威の網羅性

| 脅威 | 対応有無 |
| ----- | ----- |
| 開発時のモデル汚染 | ✅ |
| 実行時のモデル汚染 |  |
| 利用時のモデル盗難 |  |
| 学習データの汚染 |  |
| 学習データの漏洩 |  |
| 実行時のモデル盗難 |  |
| 回避（敵対的な入力に対するモデルのパフォーマンスのテスト）| ✅ |
| モデル反転/メンバーシップ推論 |  |
| モデルのサービス運用妨害 |  |
| 直接プロンプト インジェクション | ✅ |
| データの開示 |  |
| モデル入力の漏洩 |  |
| 間接プロンプト インジェクション |  |
| 開発時のモデル盗難 |  |
| インジェクション攻撃を含む出力 |  |

注:

- [開発時のモデル汚染](https://owaspai.org/goto/modelpoison/): 開発中に攻撃をシミュレートして脆弱性を評価します。
- [回避](https://owaspai.org/goto/evasion/): 敵対的な入力に対するモデルのパフォーマンスをテストします。
- [利用時のモデル盗難](https://owaspai.org/goto/modeltheftuse): 利用中のモデル悪用リスクを評価します。
- [モデル推論](https://owaspai.org/goto/modelinversionandmembership/): *メンバーシップ攻撃と反転攻撃への露出を評価します*。

### ツール名: Foolbox

<div align="center">
<table>
<tr>
	<th width="30%">要素</th>
	<th width="70%">詳細</th>
</tr>
<tr>
	<td>開発者/ソース</td>
	<td>Foolbox の作者/開発者</td>
</tr>
<tr>
	<td>GitHub リンク</td>
	<td><a href=https://github.com/bethgelab/foolbox">https://github.com/bethgelab/foolbox</a></td>
</tr>
<tr>
	<td>言語</td>
	<td>Python</td>
</tr>
<tr>
	<td>ライセンス</td>
	<td>MIT ライセンス下のオープン ソース</td>
</tr>
<tr>
	<td>軽減策の提供</td>
	<td>防止: なし、検出: あり</td>
</tr>
<tr>
	<td>API の利用</td>
	<td>可能</td>
</tr>
<tr>
	<td>人気</td>
	<td><ul><li>GitHub スター: 約 2,800 スター （2024 年現在）<li>GitHub フォーク: 約 428 フォーク<li>Issue 数: 約 21 件の未解決、350 件の解決済み<li>トレンド: 安定しており、学術コミュニティからの継続的な更新が行われています。</ul></td>
</tr>
<tr>
	<td>コミュニティ サポート</td>
	<td><ul><li>アクティブな問題: 通常、数週間以内に解決されます。<li>文書化: 基本的なチュートリアルを含む中程度の文書。より研究に重点を置いています。<li>ディスカッション フォーラム: 主に学術的な環境で議論され、業界のフォーラム活動は限定的です。<li>貢献者: 30 人を超える寄稿者（大部分は学術界出身）</ul></td>
</tr>
<tr>
	<td>スケーラビリティ</td>
	<td><ul><li>フレームワーク サポート: TensorFlow、PyTorch、JAX と互換性があります。<li>大規模なデプロイメント: 大規模な業界デプロイメントにはスケーラビリティが制限されており、研究と実験に重点を置いています。</ul></td>
</tr>
<tr>
	<td>統合</td>
	<td><ul><li>互換性:  TensorFlow、PyTorch、JAX との強力な統合。</ul></td>
</tr>
</table>
</div>

#### ツールの評価

| 基準 | 評価 |
| ----- | ----- |
| 人気 | 中 |
| コミュニティ サポート | 中 |
| スケーラビリティ | 低 |
| 統合の容易さ | 中 |

#### データのモダリティ

| データのモダリティ | サポート有無 |
| ----- | ----- |
| 文章 | ✅ |
| 画像 | ✅ |
| 音声 |  |
| 映像 |  |
| 表形式のデータ |  |

#### 機械学習タスク

| タスクの種類 | データのモダリティ | サポート有無 |
| ----- | ----- | ----- |
| 分類 | すべて（「データのモダリティ」セクションを参照） | ✅ |
| 物体検出 | コンピューター ビジョン | ✅ |
| 音声認識 | 音声 |  |

#### フレームワークの適用性

| フレームワーク/ツール | カテゴリ | サポート有無 |
| ----- | ----- | ----- |
| Tensorflow | 深層学習、生成 AI | ✅ |
| Keras	| 深層学習、生成 AI | ✅ |
| PyTorch	| 深層学習、生成 AI | ✅ |
| MxNet	| 深層学習 |  |
| Scikit-learn | 機械学習 |  |
| XGBoost | 機械学習 |  |
| LightGBM | 機械学習 |  |
| CatBoost | 機械学習 |  |
| GPy | 機械学習 |  |

#### OWASP AI Exchange の脅威の網羅性

| 脅威 | 対応有無 |
| ----- | ----- |
| 開発時のモデル汚染 |  |
| 実行時のモデル汚染 |  |
| 利用時のモデル盗難 |  |
| 学習データの汚染 |  |
| 学習データの漏洩 |  |
| 実行時のモデル盗難 |  |
| 回避（敵対的な入力に対するモデルのパフォーマンスのテスト）| ✅ |
| モデル反転/メンバーシップ推論 |  |
| モデルのサービス運用妨害 |  |
| 直接プロンプト インジェクション |  |
| データの開示 |  |
| モデル入力の漏洩 |  |
| 間接プロンプト インジェクション |  |
| 開発時のモデル盗難 |  |
| インジェクション攻撃を含む出力 |  |

注:

- [回避](https://owaspai.org/goto/evasion/): 敵対的な入力に対するモデルのパフォーマンスをテストします。

### ツール名: DeepSec

### ツール名: TextAttack

## 生成 AI レッド チーム演習のためのオープン ソース ツール

このサブセクションでは、生成 AI のセキュリティ テストに使用するツール（PyRIT、Garak、Prompt Fuzzer、Guardrail、および Promptfoo）について説明します:。

生成 AI テスト ツールのリストは、[OWASP GenAI security project solutions のページ](https://genai.owasp.org/ai-security-solutions-landscape/)（カテゴリ 'Test & Evaluate' をクリック）でも参照できます。このプロジェクトでは、[GenAI Red Teaming Guide](https://genai.owasp.org/resource/genai-red-teaming-guide/) も公開されています。

### ツール名: PyRIT

### ツール名: Garak

### ツール名: Prompt Fuzzer

### ツール名: Guardrail

### ツール名: Promptfoo




