# はじめに

AI システムのセキュリティ テストは、以下の 3 つの戦略に基づいています。

- **従来のセキュリティ テスト**（*ペネトレーション テスト*など）。[セキュア ソフトウェア開発](https://owaspai.org/goto/secdevprogram/)を参照してください。
- **モデルのパフォーマンス検証**（[継続的な検証](https://owaspai.org/goto/continuousvalidation/)を参照）: モデルの意図された動作を代表する入力と出力を含む検証セットを使用して、モデルが指定された受け入れ基準に従って動作するかどうかをテストします。セキュリティに関しては、データ汚染またはモデル汚染によってモデルの動作が恒久的に変更されていないかどうかを検出します。セキュリティ以外の分野では、機能の正確性、モデルのドリフトなどをテストします。
- **AI セキュリティテスト**（本セクション）: AI レッド チーム演習の一部であり、特定の攻撃をシミュレートすることで、AI モデルが特定の攻撃に耐えられるかどうかをテストします。

AI セキュリティ テストでは、敵対的な行動をシミュレートして、AI システムの脆弱性、弱点、リスクを明らかにします。従来の AI テストの焦点は機能とパフォーマンスですが、AI レッド チーム演習の焦点は標準的な検証を超え、意図的なストレス テスト、攻撃、安全対策の回避の試みなどが含まれます。レッド チームの焦点はセキュリティだけにとどまりませんが、本稿では主に「AI セキュリティのための AI レッド チーム」に焦点を当てます。

本セクションでは、予測 AI と生成 AI のそれぞれの性質、リスク、および適用方法が異なるため、AI レッド チームを区別します。開発時のサプライチェーンにおける脅威など、一部の脅威は両方の AI に共通する場合もありますが、それぞれの適用場面で顕在化する方法は大きく異なる場合があります。

AI レッド チーム演習への体系的なアプローチには、以下の重要なステップが含まれます。

- **目標と範囲の定義**: 目標の特定、組織、コンプライアンス、およびリスク管理要件との整合。
- **AI システムの理解**: モデル、使用事例、導入シナリオの詳細。
- **潜在的な脅威の特定**: 脅威モデリング、攻撃対象領域の特定、調査、脅威アクター。
- **攻撃シナリオの策定**: 攻撃シナリオとエッジ ケースの設計。
- **テスト実行**: 攻撃シナリオに対する手動または自動テストの実施。
- **リスク評価**: 特定された脆弱性とリスクの文書化。
- **優先順位付けとリスク軽減**: 修復のための行動計画の策定、軽減策の実施、残留リスクの算出。
- **修正の検証**: 修復後のシステムの再テスト。

AI セキュリティテストの詳細については、[OWASP AI Testing Guide](https://github.com/OWASP/www-project-ai-testing-guide) を参照してください。

# テストの対象となる脅威

資産、影響、攻撃対象領域に基づいた包括的な脅威と管理策のリストは、[AI セキュリティ基本要素表](https://owaspai.org/goto/periodictable/)として利用可能となっています。このセクションでは、AI レッド チーム演習（予測 AI および生成 AI システム）用のツールのリストを提供し、攻撃シナリオの作成、自動レッドチーム演習によるテスト実行、そして多くの場合、リスクスコアリングによるリスク評価といったステップを支援します。

リストされている各ツールは、AI システムの脅威情勢のサブセットに対応しています。以下に、考慮すべき主要な脅威をいくつか挙げます。

## 予測 AI

予測 AI システムは、入力データに基づいて予測または分類を行うように設計されています。例としては、不正検出、画像認識、レコメンデーション システムなどが挙げられます。

### 予測 AI に対する主要な脅威

- [回避攻撃](https://owaspai.org/goto/evasion/): これらの攻撃は、攻撃者がモデルを誤誘導する入力を作成し、タスクを誤って実行させる場合に発生します。
- [モデル盗難](https://owaspai.org/goto/modeltheftuse/): この攻撃では、モデルのパラメータまたは機能が盗まれます。これにより、攻撃者は複製モデルを作成し、それを敵対的攻撃やその他の複合的な脅威を作成するためのオラクルとして利用できるようになります。
- [モデル汚染](https://owaspai.org/goto/modelpoison/): これは、学習フェーズ（開発フェーズ）中にデータ、データパイプライン、またはモデル学習のサプライチェーンを操作することです。攻撃者の目的は、モデルの動作を変更し、望ましくないモデルの動作を引き起こすことです。

## 生成 AI

生成 AI システムは、テキスト、画像、音声などの出力を生成します。例としては、ChatGPT のような大規模言語モデル（LLM）や、DALL-E や MidJourney のような大規模視覚モデル（LVM）が挙げられます。

### 生成 AI に対する主要な脅威

- [プロンプト インジェクション](https://owaspai.org/goto/promptinjection/): この種類の攻撃では、攻撃者はモデルに不正な指示を与え、悪意のある結果や目的を達成しようとします。
- [実行時の直接的なモデル盗難](https://owaspai.org/goto/runtimemodeltheft/): 攻撃者はモデルの一部、またはシステムプロンプトなどの重要なコンポーネントを標的とします。これにより、ガードレールを回避する高度な入力を作成できるようになります。
- [セキュアでない出力処理](https://owaspai.org/goto/insecureoutput/): 生成 AI システムは、従来のインジェクション攻撃に対して脆弱である可能性があり、出力が適切に処理されないとリスクにつながります。

各 AI パラダイムにおける主要な脅威について説明しましたが、AI レッド チーム演習の目的と範囲の定義フェーズの結果に基づき、AI Exchange ですべての脅威を参照することを強くお勧めします。

## AI セキュリティ テストに関する参考情報

- エージェント型 AI システムのテストの詳細については、CSA と AI Exchange が共同で作成した [Agentic AI red teaming guide](https://cloudsecurityalliance.org/download/artifacts/agentic-ai-red-teaming-guide) を参照してください。
- [OWASP AI Testing Guide](https://github.com/OWASP/www-project-ai-testing-guide)

# AI および 生成 AI 向けのレッド  チーム演習ツール

以下のマインド マップは、AI レッド チーム演習のためのオープン ソース ツールの概要を示しています。予測 AI 向けのレッド チーム演習と生成 AI 向けのレッド チーム演習の 2 つに分類し、ART、Armory、TextAttack、Promptfoo などの例を取り上げています。これらのツールは現時点で利用可能な機能を示したものですが、網羅的ではなく、重要度順にランク付けされているわけでもありません。今後、新たなツールや手法が登場し、この分野に統合される可能性があります。

![attackstotesttools.png](./assets/attackstotesttools.png)

以下の図は、AI システムにおける脅威を分類し、それらの脅威に対処するために設計された関連するオープン ソース  ツールにマッピングしています。

![testtoolstoattacks.png](./assets/testtoolstoattacks.png)

以下のセクションでは、予測 AI 用のツールについて説明し、その後に生成 AI 用のセクションに進みます。

## 予測 AI レッド チーム演習のためのオープン ソース ツール

このサブセクションでは、予測 AI のセキュリティ テストに使用できるツール Adversarial Robustness Toolbox (ART)、Armory、Foolbox、DeepSec、TextAttack について説明します。

### ツール名: Adversarial Robustness Toolbox (ART)

<div align="center">
<table>
<tr>
	<th width="30%">要素</th>
	<th width="70%">詳細</th>
</tr>
<tr>
	<td>開発者/ソース</td>
	<td>IBM Research / the Linux Foundation AI & Data Foundation (LF AI & Data)</td>
</tr>
<tr>
	<td>GitHub リンク</td>
	<td><a href="https://github.com/Trusted-AI/adversarial-robustness-toolbox">https://github.com/Trusted-AI/adversarial-robustness-toolbox</a></td>
</tr>
<tr>
	<td>言語</td>
	<td>Python</td>
</tr>
<tr>
	<td>ライセンス</td>
	<td>MIT ライセンス下のオープン ソース</td>
</tr>
<tr>
	<td>軽減策の提供</td>
	<td>防止: なし、検出: あり</td>
</tr>
<tr>
	<td>API の利用</td>
	<td>可能</td>
</tr>
<tr>
	<td>人気</td>
	<td><ul><li>GitHub スター: 約 4,900 スター （2024 年現在）<li>GitHub フォーク: 約 1,200 フォーク<li>Issue 数: 約 131 件の未解決、761 件の解決済み<li>トレンド: 敵対的堅牢性のための継続的なアップデートと業界での採用により、着実に成長しています。</ul></td>
</tr>
<tr>
	<td>コミュニティ サポート</td>
	<td><ul><li>アクティブな問題: 対応力の高いチームで、通常は 1 週間以内に問題に対処します。<li>文書化: IBM の Web サイトに包括的なガイドと API 文書が用意されており、詳細かつ定期的に更新されます。<li>ディスカッション フォーラム: 主に学術的な環境で議論され、Stack Overflow や GitHub でも一部議論されています。<li>貢献者: IBM の研究者や外部の協力者を含む 100 人を超えます。</ul></td>
</tr>
<tr>
	<td>スケーラビリティ</td>
	<td><ul><li>フレームワーク サポート:すぐに使用できるサポートにより、TensorFlow、Keras、PyTorch に拡張できます。<li>大規模なデプロイメント: 医療、金融、防衛などの業界における大規模なエンタープライズ レベルのデプロイメントに対応できることが実証されています。</ul></td>
</tr>
<tr>
	<td>統合</td>
	<td><ul><li>互換性: TensorFlow、PyTorch、Keras、MXNet、Scikit-learn で動作します。</ul></td>
</tr>
</table>
</div>

#### ツールの評価

| 基準 | 評価 |
| ----- | ----- |
| 人気 | 高 |
| コミュニティ サポート | 高 |
| スケーラビリティ | 高 |
| 統合の容易さ | 高 |

#### データのモダリティ

| データのモダリティ | サポート有無 |
| ----- | ----- |
| 文章 | あり |
| 画像 | あり |
| 音声 | あり |
| 映像 | あり |
| 表形式のデータ | あり |

#### 機械学習タスク

| タスクの種類 | データのモダリティ | サポート有無 |
| ----- | ----- | ----- |
| 分類 | すべて（「データのモダリティ」セクションを参照） | あり |
| 物体検出 | コンピューター ビジョン | あり |
| 音声認識 | 音声 | あり |

#### フレームワークの適用性

| フレームワーク/ツール | カテゴリ | サポート有無 |
| ----- | ----- | ----- |
| Tensorflow | 深層学習、生成 AI | あり |
| Keras	| 深層学習、生成 AI | あり |
| PyTorch	| 深層学習、生成 AI | あり |
| MxNet	| 深層学習 | あり |
| Scikit-learn | 機械学習 | あり |
| XGBoost | 機械学習 | あり |
| LightGBM | 機械学習 | あり |
| CatBoost | 機械学習 | あり |
| GPy | 機械学習 | あり |
