# 1. はじめに	
    
## AI の信頼性の基盤としての AI テスト	
    
人工知能（AI）は、革新的な技術から現代のデジタル インフラの重要な構成要素へと進化を遂げました。AI システムは現在、医療、金融、モビリティ、公共サービス、そしてエンタープライズ オートメーションといった分野において、重要な意思決定を支えています。これらのシステムの適用範囲と自律性が拡大するにつれ、組織は AI が意図したとおりに安全に動作することを検証するための、標準化された反復可能な方法を必要としています。
    
OWASP AI Testing Guide は、AI システムの信頼性テストのための実用的な標準を確立することで、このギャップを埋めます。この標準は、セキュリティ上の脅威だけでなく、規制に準拠した責任ある AI デプロイメントに求められる、より広範な信頼性特性を評価する、技術に依存しない統一的な手法を提供します。
    
AI に対するテストは、もはやセキュリティのみに焦点を当てたものではなく、自律型および半自律型システムにおける信頼性の維持に重点を置いた、学際的な分野となっています。OWASP AI Testing Guide は、これまで欠けていた標準、すなわち、実際の攻撃パターン、新たなグローバル標準、そして AI セキュリティ コミュニティの実体験に基づいた、AI システムの信頼性テストのための統一的、実用的、かつ包括的なフレームワークを確立します。
    
## AI テストが独特な理由	
    
従来のソフトウェア テストは、不正アクセス、コードの欠陥、システムの脆弱性からシステムを保護することに重点を置いています。AI システムでは、それ以上の対策が求められます。AI モデルは非決定論的な方法で学習、適応、一般化、そして失敗するため、従来のセキュリティ テストでは対処できないリスクが生じます。
    
NIST AML Taxonimy と OWASP Top 10 for LLM Applications 2025 に記載されている証拠から、AI システムが失敗する理由はセキュリティをはるかに超えるものであることがわかります。
    
- 敵対的操作（プロンプト インジェクション、脱獄、モデル回避）
- バイアスと公平性の欠陥
- 機密情報の漏洩
- 幻覚と誤情報
- サプライチェーン全体にわたるデータ/モデルの汚染
- 過剰または安全でない主体性
- ユーザーの意図または組織ポリシーとの不一致
- 不透明または説明不能な出力
- 時間の経過に伴うモデルのドリフトと劣化
    
こうした複雑さから、業界は「セキュリティだけでは不十分であり、AI の信頼の置ける性質（Trustworthiness）こそが真の目的である」という原則に収束しつつあります。この OWASP AI Testing Guide は、これらの原則を実用的なテスト フレームワークとして運用化しています。
    
AI モデルは、巧妙に作成された入力（敵対的サンプル）によって欺かれたり、操作されたりする可能性があるため、組織は標準的な機能テストをはるかに超える、専用の敵対的堅牢性テスト手法を採用する必要があります。こうした専門的なセキュリティ評価がなければ、AI システムは、整合性、信頼性、そして全体的な信頼性を損なう可能性のある、微妙な攻撃に対して脆弱なままです。
    
## OWASP AI Testing Guide の目的と範囲	
    
OWASP AI Testing Guide は、以下を提供します。
    
- AI および LLM ベースのシステムの信頼性テストのための標準化された方法論
- 以下のリスクを評価する反復可能なテスト ケース
    - AI アプリケーション層
    - AI モデル層
    - AI インフラストラクチャ層
    - AI データ層
    
このガイドは、ソフトウェア開発者、アーキテクト、データ アナリスト、研究者、監査人、リスク担当者のための包括的参照情報として設計されており、製品開発ライフサイクル全体を通じて AI リスクが体系的に対処されることを保証します。
    
このガイダンスに従うことで、チームは、潜在的なバイアス、脆弱性、パフォーマンスの低下が事前に特定され、軽減されているという検証可能な保証を得て、AI システムを自信を持って本番環境にデプロイするために必要なレベルの信頼を確立できます。
    
## 1.1. 序文と寄稿者	
    
OWASP AI Testing Guide
    
AI システムの信頼性テストのための標準 **バージョン 1.0 – 2025 年 11 月**
    
OWASP AI Testing Guide は、CC BY-SA 4.0 ライセンスに基づいて公開されています。
    
### 序文	
    
人工知能（AI）はソフトウェアの設計、デプロイ、そして防御の方法を変革していますが、AI システムのテスト、検証、そして保証の能力は、同じペースで進化していません。従来のアプリケーション セキュリティ テストは、学習、適応、そして予測不可能な動作をするモデルによって駆動されるシステムに対しては、もはや十分ではありません。
    
2023 年、OWASP は *Top 10 for Large Language Model Applications* を発表しました。これは、一般的な AI リスクをマッピングする初のグローバルな取り組みです。**OWASP AI Testing Guide (AITG)** は、次のステップとして、データ収集とモデルの学習から、デプロイ、監視、実行時動作に至るまで、**AI システムのライフサイクル全体にわたって信頼性を評価するための、構造化され、反復可能で、コミュニティ主導**の手法を提供します。
    
このガイドは、AI テスター、機械学習エンジニア、リスク管理者、監査担当者など、高レベルの AI ガバナンス原則を実用的かつテスト可能な管理策に落とし込む必要がある方々を対象としています。各テスト ケースは、目標、ペイロード、そして観察可能なレスポンスを修復ガイダンスにリンクさせ、一貫した評価とエビデンスに基づくレポート作成を可能にします。
    
**バージョン 1.0** では、OWASP AI Testing Framework を構成する以下の 4 つのテスト カテゴリが導入されています。
    
1. AI アプリケーションのテスト – プロンプト、インターフェース、統合ロジックの検証。
2. AI モデルのテスト – モデルの堅牢性、アライメント、敵対的耐性の調査。
3. AI データのテスト – データの整合性、プライバシー、出所の評価。
4. AI インフラストラクチャのテスト – パイプライン、オーケストレーション、実行時環境の評価。
    
各カテゴリは、以下の一貫したプロセスに従います。
    
> 目標の定義→テストの実施→応答の解釈→修復の推奨
    
AI Testing Guide は、特定のツールを規定するのではなく、AI システムのレジリエンス（回復力）を測定するための方法論の標準、つまり共通言語を定義します。このフレームワークは、実世界におけるテスト、学術研究、そしてコミュニティからのフィードバックに基づき、継続的に進化するように設計されています。GitHub の Issue、プル リクエスト、そしてコミュニティの議論を通して、皆様の貢献をお待ちしております。共に、**AI を設計段階から信頼できるものにしていく**ために、ご協力をお願いします。
    
**OWASP Foundation**、*Top 10 for LLM Applications* と *GenAI Red Teaming Guide* の貢献者、そして NIST AI RMF と AI 100-2e チームの皆様に、その基礎的な取り組みに感謝します。そして何よりも、AI システムのオープンな環境でのテスト、攻略、そして強化に時間を割いてくださっているOWASP コミュニティと実務者の皆様に感謝します。
    
Onward,<br>
Matteo Meucci & Marco Morana<br>
*OWASP AI Testing Guide プロジェクト共同リーダー*

### AI Testing Guide の著者および貢献者	
    
プロジェクトに関わったすべての人々に感謝します。
    
- 著者

    Julio Araujo • Roei Arpaly • Isaac Bentley • Yoni Birman • Henriette Cramer • Luca Demetrio • DotDotSlash • Federico Dotta • Didier Durand • Almog Langleben • Grao Melo • Matteo Meucci • Marco Morana • Maura Pintor • Jeremy Redmond • Federico Ricciuti • Mart Jord Roca • Sita Ram Sai • Dhanith Krishna • Nicolas Humblot
    
- 貢献者

    Jacob Beers • Giovanni Cerrato • Fabio Cerullo • Andrea Fukushima • Sebastien Gioria • Joey Melo • Kunal Sinha • Cecil Su • Melvin Tan
    
謝辞: このプロジェクトの実現にあたり、フィードバック、アイデア、そして励ましを共有してくださった OWASP AITG コミュニティの皆様、特に Slack チャンネルの皆様に感謝します。皆様のご意見は、このプロジェクトの発展に大きく貢献しました。
    
## 1.2. OWASP AI Testing の原則	
    
信頼できる AI は、**責任ある AI (RespAI)**、**セキュリティが確保された AI (SecAI)**、**プライバシーに配慮した AI (PrivacyAI)** という 3 つの基盤領域の強みを組み合わせることで実現されます。これらの領域は、**OWASP AI Testing Framework** における信頼できる AI のテスト可能な基盤を形成します。信頼できる AI のより広い定義には、ガバナンス、信頼性 (reliability)、説明責任性も含まれますが、これらの特性は、以下の 3 つの領域にわたる**継続的なテスト**を通じて実現され、運用されます。
    
効果的な AI テストは、以下の側面を包括的に統合します。
    
- **セキュリティ**は、敵対的およびインフラストラクチャに対する脅威に対する耐性を確保します。
- **プライバシー**は、機密性を保護し、機密データの悪用や推測を防止します。
- **責任ある AI** は、倫理的で透明性があり、バイアス耐性のある行動を施行します。
    
これらが組み合わさることで、**信頼できる AI システム**（安全かつ予測可能で、人間の価値観に沿って動作するシステム）を検証、制御、維持するための統一された構造が形成されます。
    
### 1. セキュリティが確保された AI (SecAI)	
    
AI システムは、敵対的な脅威やシステム的な悪用に対して耐性を持ち、AI スタックとライフサイクル全体にわたって保護を確保する必要があります。
    
- **プロンプトと入力の制御**: システム プロンプト、指示、およびユーザー入力をインジェクションや不正操作から保護します。
- **敵対的堅牢性**: 回避、汚染、モデル盗難、脱獄、間接プロンプト インジェクションに対する耐性をテストします。
- **インフラストラクチャ セキュリティ**: API エンドポイント、プラグイン、RAG パイプライン、およびエージェント ワークフローの脆弱性を評価します。
- **サプライチェーン リスク**: モデルと依存関係に汚染、改ざん、またはサードパーティによる侵害がないか検査します。
- **継続的なテスト**: CI/CD パイプラインに敵対的および依存関係の自動スキャンを統合します。
    
### 2. プライバシーに配慮した AI (PrivacyAI)	
    
モデルのライフサイクル全体を通じて、AI システムに公開されるデータや AI システムによって生成されるデータの機密性とユーザーのデータ管理権限を確保します。
    
- **データ漏洩の防止**: 学習データ、プライベート コンテキスト、またはユーザー入力の意図しない漏洩を検出します。
- **メンバーシップおよびプロパティ推論への耐性**: データが学習の一部であるかどうかを推論する攻撃に対する脆弱性を評価します。
- **モデルの抽出と持ち出し**: 独自のモデルや重みを複製しようとする攻撃者をシミュレートします。
- **データ ガバナンスのコンプライアンス**: 最小化、目的の制限、および同意管理の原則の遵守を検証します。
    
### 3. 責任ある AI (RespAI)	
    
継続的な評価と軽減を通じて、倫理的かつ安全で整合性のあるシステム動作を促進します。
    
- **バイアスと公平性の監査**: 人口統計グループやエッジ ケース全体にわたって、差別的な出力を特定します。
- **有害性と不正使用の検出**: 有害または誤解を招くコンテンツの作成または拡散に対する耐性をテストします。
- **安全性の整合（アライメント）**: 整合制約の遵守と、脱獄やロール プレイングによるエクスプロイトへの耐性を検証します。
- **ガードレールの網羅性**: 安全フィルター、拒否メカニズム、不正使用防止ロジックを評価します。
- **人間参加型（HITL）の管理策**: 影響の大きい意思決定については、エスカレーションを確実に実施し、経路をレビューします。
    
### 4. 信頼できる AI システム	
    
**信頼できる AI (Trustworthy AI) = 責任ある AI (RespAI) + セキュリティが確保された AI (SecAI) + プライバシーに配慮した AI (PrivacyAI)** であり、長期にわたって信頼を維持するガバナンス、透明性、監視の機構によってサポートされます。
    
- **説明可能性**: ユーザーと監査者が意思決定の方法と理由を理解できることを確実にします。
- **一貫性と安定性**: プロンプトの変動と回帰テストにおいて、予測可能な応答を検証します。
- **継続的な監視**: 実行時の可観測性、ドリフト検出、自動異常アラートを適用します。
- **ライフサイクル テスト**: 設計からデプロイメント、そして市場投入後の段階まで検証を拡張します。
- **ポリシーと規制との整合（アライメント）**: テストと検証のプロセスを、NIST AI RMF<sup>[1]</sup>、ISO/IEC 42001<sup>[2]</sup>、OWASP Top 10 for LLM<sup>[3]</sup> などのフレームワークにマッピングします。
    
効果的な AI テストは、信頼できる AI システムを構築するために、セキュリティ、プライバシー、責任ある AI という3つの主要な領域を基盤としています。私たちがこれら 3 つのコア領域を選んだのは、これらが AI リスクのあらゆる側面に総合的に対処できるからです。セキュリティは、敵対的脅威やインフラストラクチャの脅威に対する耐性を確保します。プライバシーは、意図しないデータ漏洩や推論攻撃を防ぎます。責任ある AI は、倫理的な行動と公平性を重視し、バイアスや悪用を防ぎます。これらが一体となって、安全で信頼性の高い AI デプロイメントを検証、管理、維持するための包括的なフレームワークを形成します。各領域には、最新の AI アプリケーションの評価を導く重要な原則が含まれています。
    
### AI をテストするタイミング	
    
ISO/IEC 23053<sup>[4]</sup> は、ML ベースの AI システムのライフサイクルを、それぞれ明確な目標、成果物、ガバナンスのタッチポイントを持つ一連の繰り返し可能なフェーズに構造化しています。
    
1. **計画と適用範囲の設定**: このフェーズでは、明確なビジネス目標、成功指標、ML 使用事例を確立するとともに、主要なステークホルダー、規制要件、組織のリスク許容度を特定します。
2. **データの準備**: このフェーズでは、生データ ソースを収集して文書化し、前処理パイプラインを通じてプロファイリングと品質チェックを実施し、データの完全な追跡可能性を確保するためにバージョン管理と系譜追跡を実装します。
3. **モデルの開発と学習**: このフェーズでは、適切なアルゴリズムとアーキテクチャを選択し、特徴量エンジニアリングを用いてキュレーションされたデータセットでモデルを学習させ、学習プロセスを制御するパラメータ（ハイパー パラメータなど）やパフォーマンス指標を含む実験をモデル レジストリに記録します。
4. **検証と評価**: このフェーズでは、予約済みデータセットと敵対的データセットを使用してモデルをテストし、公平性、堅牢性、セキュリティの評価を実施し、機能、倫理、規制基準を満たしていることを確認します。
5. **デプロイメントと統合**: このフェーズでは、学習済みの AI モデルを準備し、サービス（マイクロサービスまたは API でモデルをラップする）またはエッジ デプロイメント（IoT ゲートウェイやモバイル フォンなどのリソースが限られたデバイス向けにモデルを変換および最適化する）用のデプロイ可能なアーティファクトにバンドルします。CI/CD を介してビルド、テスト、リリースのワークフローを自動化し、インフラストラクチャのセキュリティ対策を検証します。
6. **運用と保守**: このフェーズでは、AI 製品が本番環境にある間、パフォーマンス、データ ドリフト、監査ログを継続的に監視し、異常やコンプライアンス違反が発生した場合にアラートをトリガーします。同時に、定期的に最新データでモデルを再学習させ、セキュリティ、プライバシー、公平性に関する管理策を再検証し、必要に応じて文書化、学習、ポリシーを更新します。
    
AI システムが開始から継続的な運用に至るまで、正確性、安全性、公平性、信頼性を維持できるように、AI テストを AI システムのライフサイクル全体に統合する必要があります。
    
1. **計画と適用範囲の設定**:ビジネス目標、成功指標、機械学習の使用事例がテスト可能かつ追跡可能であることを確認します。AI 固有のリスク（敵対的リスク、プライバシー リスク、コンプライアンス リスク）を特定し、それらを管理策にマッピングします。ステークホルダーの役割、規制上の制約、リスク許容基準が文書化されていることを検証します。
2. **データの準備**: データ品質テストを実施し、欠損値、外れ値、スキーマの不一致、重複がないか確認します。特徴量分布（特定の変数の値がどのように分布または配置されているか）を過去のプロファイルと比較検証し、ドリフトしきい値（このベースラインからのデータ ドリフト）を設定します。すべてのデータソース、変換、バージョンが記録され、追跡可能であることを確認します。
3. **モデルの開発と学習**: 前処理コード、カスタム レイヤー、および特徴量エンジニアリング関数が期待どおりに動作することを検証します。モデル コードに対して静的コード スキャン（SAST など）を実行し、セキュアでない依存関係や構成ミスがないか確認します。学習、検証、テストの分割間でデータ漏洩がないことを確認します。チューニングの変更によって、回帰なしに汎化が向上することを確認します。
4. **検証と評価**: ホールド アウト テスト セットと敵対的テスト セットにおいて、正解率、適合率／再現率、AUC などを測定するベンチマークに対するパフォーマンス検証を実施します。公平性とバイアスの監査を実施し、人口統計スライスとエッジ ケース全体にわたってモデル出力を評価します。ニューラル ネットワークやその他の敵対的攻撃に対する敵対的サンプルを作成するための既知の手法を適用することで、敵対的堅牢性テストを実施し、耐性を評価します。プライバシー攻撃を実施し、メンバーシップ推論、モデル抽出、汚染をシミュレートすることで、プライバシー保護を確認します。予測結果を入力特徴に帰属させることで、モデルの決定が解釈可能かつ妥当であることを検証します。
5. **運用と保守**: ドリフト検出のための回帰テストを実施し、本番環境の入出力を検証ベースラインと継続的に比較します。パフォーマンスの低下、データ ドリフト、またはセキュリティ異常が発生した場合に、監視ルールが正しく実行されることを確認します。モデルの更新またはデータ更新後には、パフォーマンス、公平性、堅牢性を再評価します。セキュリティ、プライバシー、倫理に関する管理策が引き続き有効であり、文書化されていることを定期的に確認します。
    
このガイドのテスト目標の 1 つは、OWASP の LLM 固有のテスト ケースと、より広範な OWASP AI Exchange<sup>[5]</sup> の脅威をライフサイクル フェーズに統合し、リリース前の検証と新たな脆弱性に対する継続的な保護を確実にすることです。例えば、計画および適用範囲の設定のフェーズでは、脅威モデリング演習を使用して、OWASP Top 10 LLM リスク（プロンプト インジェクション、データ漏洩、モデル汚染、過度の依存など）と AI Exchange の脅威を列挙し、テストの範囲とコントロールを定義することができます。
    
例えば、検証と評価のフェーズでは、プロンプト インジェクションのテストで直接的および間接的なプロンプト操作をテストしてガードレールの網羅性と拒否動作を確認し、制御された再学習ループに悪意のあるサンプルを挿入して汚染への管理策が機能することを確認できます。開発および運用中は、新しくインストールまたは更新されたプラグインを継続的にスキャンして OWASP で特定された脆弱性を探し、脱獄、バックドア プロンプト、または既知の OWASP AI Exchange 脅威ベクトルの悪用の兆候がないか出力を監視するようにテストを指示できます。
    
この最初のリリースでは、OWASP AI Testing Methodology は、AI 製品所有者がテスト範囲を定義し、最初の AI 製品バージョンがテスト準備が整ったら包括的な評価スイートを実行するためのガイダンスに重点を置いています。将来のアップデートでは、このガイダンスが拡張され、より初期の製造前段階もカバーされる予定です。

## 1.3. AI Testing Guide の目標

OWASP は長年にわたり、Web アプリケーションのセキュリティ ガイドライン策定をリードしてきましたが、そのリーダーシップを AI 分野にも拡大します。AI Testing Guide は、AI システムの堅牢性、信頼の置ける性質（trustworthiness）、そしてレジリエンス（回復力）を評価するための、構造化され実践的なフレームワークを提供します。AI セキュリティ テスター、監査担当者、レッド チームの専門家、MLOps エンジニア、そして開発者が、AI アプリケーション、モデル、インフラストラクチャ、そしてデータ パイプラインがもたらす固有のリスクを特定、モデル化、検証できるよう支援します。

### 本ガイドの対象読者

オープン コミュニティのコラボレーションを通じて世界中のソフトウェア セキュリティを向上させるという OWASP の使命に沿って、AI Testing Guide は、以下の人々を対象としています。

- 標準的な脆弱性スキャンの枠を超え、モデルの挙動と敵対的耐性を詳細に評価したい AI セキュリティ テスター。
- AI システムが責任ある AI 原則と業界規制を満たしていることを検証する AI 監査担当者およびコンプライアンス チーム。
- 回復力があり信頼のおける AI パイプラインとサービスを構築するための実践的で実用的なガイダンスを求めている AI エンジニア、開発者、MLOps 専門家。
- 敵対的評価や生成 AI レッドチーム演習を実施し、潜在的な脆弱性を発見したい AI レッド チーム。

上記の役割に加え、OWASP AI Testing Guide は、従来のセキュリティ チームをはるかに超えて、製品オーナー、リスクおよびガバナンス担当者、QA エンジニア、DevSecOps 実践者、インシデント対応者、学術研究者などを支援することを目的としています。 OWASP のオープン コラボレーション モデルの下でこの多様なコミュニティを統合することにより、世界的な専門知識を活用して、世界中の AI セキュリティの水準を引き上げることに取り組んでいます。

### 方法論: 脅威モデリングからテストまで

OWASP AI Testing Guide では、脅威主導型の方法論を採用しています。AI システムは、敵対的なエクスプロイトからプライバシー侵害に至るまで、多岐にわたる、影響の大きい特有のリスクを伴い、ビジネス運営やユーザーの安全に最も影響を与える可能性の高いシナリオにリソースを割り当てることが求められます。まず脅威モデリングと脅威マッピングを実施し、次に対象を絞ったテスト ケースを策定することで、すべての評価において、システム アーキテクチャとリスク許容度に最も関連性の高い AI 固有の脅威に対応していることを保証します。このガイドは、明確な方法論に基づいて構成されています。

- 脅威モデリング: まず、AI システムの高レベルな図を作成し、アプリケーション、モデル、インフラストラクチャ、データの 4 つの主要コンポーネントに分解します。このアーキテクチャ ビューは、信頼境界と、脅威が発生する可能性のある重要な相互作用を明らかにします。
- 脅威マッピング: 特定された脅威は、以下を含む既存のソースに基づいてカタログ化されています。
    - OWASP Top 10 for LLMs
    - OWASP AI Exchange
    - Responsible AI and Trustworthy AI framework
- テストの設計: マッピングされた脅威ごとに、以下の項目を規定するカスタマイズされたテスト ケースを策定します。
    - サンプル ペイロード：脅威をトリガーするように設計された具体的な入力または操作。
    - 期待される結果: 正しいシステム応答または失敗モード。
    - 検出戦略: 侵害の兆候を監視、ログ記録、またはアラートする方法。
    - ツールの推奨: 各テストに適したオープン ソースまたは商用ツール。
    
    これらの手順に従うことで、チームは AI 固有のリスクの理解から、実用的で反復可能なテストによる防御の検証まで、シームレスに移行できます。

### 本プロジェクトで網羅されない事項

このガイドは、既存の基礎的なセキュリティ テスト手法を置き換えたり、複製したりするものではありません。AI 特有の脅威に焦点を当てることで、既存の手法を補完するものです。一般的なシステムおよびアプリケーションのセキュリティについては、以下の最高水準の参考資料を推奨します。

- ネットワーク セキュリティ: NIST SP 800-115<sup>[6]</sup>、Technical Guide to Information Security Testing and Assessment
- インフラストラクチャ セキュリティ: OSSTMM<sup>[7]</sup>、Open Source Security Testing Methodology Manual
- Web アプリケーション セキュリティ: OWASP Web Security Testing Guide (WSTG)<sup>[8]</sup>
