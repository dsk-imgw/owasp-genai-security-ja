# 2. AI システムに対する脅威モデリング

## 脅威モデリングとは？

脅威モデリングとは、システムに対するセキュリティ上の脅威を特定、定量化し、対処するための構造化されたプロセスです。開発者、アーキテクト、セキュリティ専門家は、これにより、システムがどのように攻撃される可能性があるかを事前に評価し、開発ライフ サイクルの早い段階で適切な防御策を設計することができます。

AI システムにおいて、脅威モデリングは、新たに出現し高度化する脅威ベクトルを明らかにし、データ資産に対する潜在的な攻撃経路を明確化し、技術面とビジネス面の両方への影響を定量化します。これらのリスクは、プロンプト インジェクションからモデル抽出に至るまで、機械学習と生成 AI 技術の独特な特性から生じます。

## AI 脅威モデリングのコア目標

AI システムの脅威モデリングは、AI 固有の攻撃対象領域を特定し、影響度の高いリスク（敵対的攻撃や推論攻撃など）を優先順位付けし、標的を絞ったテストを導くことを目的としています。これにより、セキュア・バイ・デザイン（設計段階からセキュアなアーキテクチャ）が促進され、エンジニアリング、セキュリティ、コンプライアンスの各チーム間で共通のリスク言語が構築され、規制デュー デリジェンスのための文書化された証拠が提供されます。脅威モデルを継続的に更新することで、組織はAIコンポーネントや脅威の進化に合わせて適応する、生きたリスク ロードマップを維持できます。

目標の定義に加えて、AI 脅威モデリングには以下のことが含まれます。

- **攻撃対象領域の分析**: AI/ML システムをコンポーネント（データソース、学習パイプライン、推論エンドポイント、モデル ストア、オーケストレーション レイヤー）に分解します。データフローをマッピングし、悪意のある入力や情報漏洩が発生する可能性のある信頼境界を特定します。
- **資産とアクターの特定**: 重要な資産（学習データセット、モデル パラメーター、推論 API）と、それらを操作するユーザーまたはプロセスをカタログ化します。権限レベルと潜在的な脅威アクター（外部攻撃者、不正な内部関係者、サードパーティ サービス）を特定します。
- **脅威ライブラリのマッピング**: 既存の脅威カタログを活用し、AI 固有の攻撃を包括的にカバーします。
- **リスク分析と優先順位付け**: 各脅威の発生可能性と影響度を、技術面（モデルの完全性、可用性）とビジネス面（収益損失、風評被害）の両方から推定します。脅威をランク付けし、リスクを最も低減できる箇所にテストと軽減策を集中させます。
- **軽減戦略の定義**: 優先順位付けされた脅威ごとに、リスクを許容レベルまで低減するために必要なアーキテクチャ管理策、ランタイム防御、または運用プロセスを指定します。

## AI 脅威モデリング フレームワークの選択方法

AI システムに対する脅威を体系的に特定・分析するために、いくつかの確立された方法論を応用することができます。それぞれが、ビジネス主導のリスク中心のアプローチから、プライバシー重視の評価や敵対的攻撃マッピングまで、独自の視点をもたらします。これらのフレームワークを慎重に適用することで、チームは AI 固有の脆弱性を発見し、軽減策を優先順位付けし、AI ライフサイクル全体にわたってセキュリティを統合することができます。

以下は、AI 脅威モデリングに用いられる主要な方法論の概観です。

- **PASTA<sup>[9]</sup> (Process for Attack Simulation and Threat Analysis)**: 技術分析とビジネスへの影響を整合させる、7 段階のリスク中心のフレームワーク。
- **STRIDE<sup>[10]</sup>**: Microsoft の STRIDE モデルで、脅威をなりすまし (spoofing)、改ざん (Tampering)、否認 (Repudiation)、情報漏洩 (Information disclosure)、サービス拒否 (Denial of service)、権限昇格 (Elevation of Privilege) に分類します。
- **MITRE ATLAS<sup>[11]</sup>**: 敵対的機械学習手法（回避、汚染、モデル抽出）とそれに対応する軽減策をマッピングします。
- **LINDDUN<sup>[12]</sup>**: データの機密性とコンプライアンスに対する脅威（メンバーシップ推論、データ漏洩など）をモデル化するための、プライバシー重視のフレームワーク。
- **MAESTRO<sup>[24]</sup>**: エージェント型 AI の脅威モデリング向けに特別に設計された、新しいモデル駆動型アプローチ。MAESTRO は、Multi-Agent Environment（マルチ エージェント環境）、Security（セキュリティ）、Threat（脅威）、Risk（リスク）、Outcome（結果）の頭文字をとったものです。

以下のような、組織の目標、システムの複雑さ、そしてステークホルダーのニーズに最も適した方法論を選択してください。

- **ビジネスとリスクの整合**: セキュリティ分析を具体的なビジネス影響（例：損失エクスポージャーの定量化）に結び付けることが主な目標である場合は、PASTA のようなリスク中心のフレームワークが理想的です。
- **スコープと複雑さ**: エンドツーエンド の AI パイプラインには、幅広い段階のプロセス（PASTA、MITRE ATLAS）を使用します。個々のコンポーネントには、より簡略化された分類法（STRIDE、OWASP LLM Top 10）が適しています。
- **対象者と成熟度**: 経営幹部やリスク管理担当者は、ビジネスに焦点を当てた高レベルのアウトプット（PASTA のビジネス目標ステージ、リスク登録簿）を好む傾向があります。エンジニアリング チームは、設計パターンやコードに直接マッピングできる、開発者にとって使いやすい分類法（AI-STRIDE または MITRE ATLAS マトリックス）を好む場合があります。
- **プライバシー対セキュリティ**: データの機密性とコンプライアンスが最優先事項である場合は、プライバシー中心の手法（LINDDUN）をコア セキュリティ アプローチと併せて導入してください。敵対的攻撃に対する堅牢性が最優先事項である場合は、選択したフレームワーク（MITRE ATLAS または カスタム AI-STRIDE 拡張）に敵対的テストケース設計が含まれているか、簡単に統合できることを確認してください。
- **エージェント型 AI の脅威モデリング**: AI エージェントがユーザー、ツール、他のエージェント、またはその環境と対話するシステムにおけるリスクをモデル化する必要がある場合は、MAESTRO（注 (a)）を使用してください。これは、現実世界の AI の障害やセキュリティ問題の多くが発生するコンテキストです。
- **LLM を活用した脅威モデリング**: 大規模言語モデル（LLM）は、従来は手動で時間のかかる複数のステップを自動化することで、脅威モデリング プロセスを効率化できます。参考情報 [25] のトレーニングで扱われている LLM 拡張型脅威モデリングでは、大規模言語モデルを用いて脅威モデリング プロセスの各段階を加速・強化し、システム記述（テキスト ベースの文書、アーキテクチャ図、あるいはコードなど）から直接、脅威や緩和策、そして管理策に関する推奨事項を自動生成します。
- **ツールとプロセスの適合性**: 既存の SDLC、脅威モデリング ツール、レポート ダッシュボードと互換性のある方法論を選択してください。PASTA の各段階は、リスク管理プラットフォームで適切に機能し、LLM 脅威モデリング プロンプト テンプレート（注(b)）を用いて LLM を活用できます。STRIDE は、ThreatDragon のような手動の脅威モデリング ツールと、STRIDEGPT のような LLM を活用した脅威モデリング ツールの両方に簡単にマッピングできます。

----------------------------------
<div style="font-size: small">

- 注 (a): MAESTRO は、STRIDE、PASTA、その他の従来のフレームワークに代わるものではなく、AI 固有の脅威クラス、マルチ エージェント コンテキスト、およびライフサイクル全体のセキュリティの考慮事項を追加することで、それらを補完します。
- 注 (b): 特別に設計されたプロンプト テンプレートを使用することで、LLM を用いた脅威モデリング プロセスを強化できます。STRIDE および PASTA LLM 脅威モデリング プロンプト テンプレートの例は、参考文献 [26] でいくつか公開されています。これらのテンプレートは、大規模言語モデル（LLM）が脅威モデリング タスクを一貫性と精度をもって実行できるように、再利用可能な構造化されたプロンプトを提供します。
</div>

## AI システムのアーキテクチャ

脅威を包括的な AI アーキテクチャにマッピングすることが重要です。(*) 脅威はシステム設計に依存するため、AI システムの各部分（データ取り込み、学習パイプライン、モデル API、監視システム）にはそれぞれ異なる脆弱性があります。アーキテクチャを完全に可視化できなければ、重要な攻撃対象領域を見逃してしまう可能性があります。脅威を特定のコンポーネントにマッピングすることで、脅威が実際に発生する可能性のある場所を特定し、システムをブラック ボックスとして扱うのではなく、リスクの優先順位付けを行うことができます。脅威をアーキテクチャ全体にマッピングすることで、境界だけでなく、各重要な境界（データ、モデル、API、インフラストラクチャ）に階層化されたセキュリティ管理策を設計できます。脅威のマッピングは、構造化された脅威モデリング（AI 向けの STRIDE、PASTA、LINDDUN など）を体系的にサポートし、具体的で実用的な対策を容易に設計できるようになります。脅威モデリングはスコープとコンテキストに大きく依存するため、最も一般的なAI脅威を反映し、今日のほとんどの AI アプリケーションの基盤となる技術的およびビジネス的な使用事例と一致するアーキテクチャ範囲を選択することが重要です。

PASTA のステージ II では、Secure AI Framework（SAIF）<sup>[12]</sup>と整合させてアーキテクチャの範囲を定義し、AI システムのコア セキュリティ関連コンポーネントの構造化されたビューを確立します。SAIF は、大規模な AI システムのセキュリティを確保するための公開モデルとして機能し、AI システムのセキュリティをより広範なリスク管理および運用回復力の目標に結び付ける、実用的で適応性に優れたビジネスに合わせたフレームワークを提供します。特に、SAIF Risk Map<sup>[13]</sup> は、AI セキュリティをナビゲートするための視覚的なガイドとして機能し、包括的なセキュリティ フレームワークとしての SAIF を理解する上で中心的な役割を果たします。このマップは、プロンプト インジェクション、データ汚染、不正なアクションなど、開発者には馴染みのない多くのリスクを強調しています。AI 開発プロセスをマッピングすることで、SAIF マップはこれらのリスクが発生する場所を特定し、重要なことに、対応するセキュリティ管理策を適用できる場所を特定するのに役立ちます。図 1 に、SAIF コンポーネントの図を示します。

<div align="center">

![](./assets/AISystemArchitecture.png)

図 1: SAIF アーキテクチャの層とコンポーネント
</div>

SAIF マップ図は、AI セキュリティをアプリケーション、モデル、インフラストラクチャ、データの 4 つの主要領域に分類し、AI 開発ライフサイクル全体にわたる AI 保護の範囲を網羅しています。上半分は、モデルのデプロイメントとユーザー インタラクションまでのパスを示し、AI を活用したアプリケーションを構築するモデル利用者に最も関連性の高いリスクと管理策に焦点を当てています。SAIF マップ図の下半分は、モデルの開発プロセスを示し、モデル作成者、つまり自組織または他組織向けにモデルを学習または微調整する担当者に焦点を当てています。AI の使用方法によっては、より関連性の高いリスクが異なる場合があります。

SAIF Risk Map は、AI 開発ライフサイクルにおいて、多くの場合、人、プロセス、またはツールの脆弱性によってリスクが導入される場所、リスクが露出する場所（つまり、セキュリティ チームが観察またはテストできる場所）、そして適切な管理策の実装によって最終的にリスクを軽減できる場所を示しています。これらのリスク パスの一部は、主にモデル利用層（アプリケーションとモデル）と関連する AI コンポーネントに現れ、その他はモデル作成層（インフラストラクチャとデータ）に現れ、多くは両方にまたがるため、AI システムのライフサイクル全体にわたる包括的なセキュリティ網羅性の必要性が強調されます。

Google の Secure AI Framework（SAIF）を採用することで、すべてのサブ コンポーネント（RAG や記憶モジュールなど）を分解するのではなく、データ、学習、推論、デプロイメントという最上位ドメインに焦点を当てています。SAIF の構造との整合により、モデルの明確性が維持され、不要な複雑さを回避できます。


## AI システムのアーキテクチャの脅威モデリング

脅威モデリング手法の選択にあたっては、技術分析とビジネスへの影響を整合させるリスク中心のフレームワークとして、PASTA、MITRE ATLAS（敵対的テスト ケース設計を含む、または容易に統合可能なフレームワーク）、そして開発者にとって分かりやすい脅威分類のための STRIDE と STRIDE AI など、主要なアプローチをいくつか紹介します。どの手法を選択しても、望まれる本質的な成果は同じです。それは、アプリケーション、データ、モデル、インフラストラクチャといったコア コンポーネントを網羅し、SAIF Risk Map で定義された参照 AI アーキテクチャに脅威を体系的にマッピングすることです。

目標は、STRIDE の脅威カテゴリと AI 固有の脅威の両方を活用した包括的な脅威分析を実施することです。特定された脅威は、脅威駆動型および攻撃駆動型のテストを設計するための基盤となり、管理策ギャップ、弱点、脆弱性を明らかにするために使用されます。これらの（技術的およびビジネス的）脆弱性の発生可能性と影響は、それに応じて評価および軽減する必要があるリスクの次元を形成します。

PASTA のようなリスク ベースの手法を採用する場合、そのプロセスはビジネス目標の定義から始まります。つまり、重要な機能（パーソナライズされた推奨、予測分析、自律的な意思決定など）を提供する AI 活用サービスを保護しながら、顧客の信頼の喪失、規制上の罰則、モデルの侵害による競争上の脅威といったリスクを軽減することです。

PASTA のステージ 2 では、SAIF Risk Map を用いて、ユーザー、アプリケーション、モデル、データ、インフラストラクチャを網羅する包括的な技術的範囲を確立します。PASTA のステージ 3 では、アーキテクチャを 4 つの主要レイヤーに分解し、高レベルのデータフローをマッピングします。PASTA のステージ 4 では、AI 固有の脅威に対する詳細な脅威分析を実施します。PASTA のステージ 5 と 6 では、標的を絞ったテストと現実的な攻撃シナリオのシミュレーションを通じて、脆弱性を特定することに重点を置きます。最後に、PASTA のステージ 7 では、軽減されていないリスクの重大度を評価し、業界のベスト プラクティスに基づいて軽減戦略の概要を策定します。

一方、STRIDE を主要な脅威モデリング手法として直接採用する場合は、プロセスが異なります。STRIDE は、PASTA のような完全なエンド ツー エンドのリスク中心の脅威モデリング手法ではなく、脅威分類フレームワークとして機能するためです。このアプローチでは、6 つの STRIDE 脅威カテゴリを SAIF アーキテクチャ コンポーネント（アプリケーション、モデル、データ、インフラストラクチャ）全体に体系的に適用することで、包括的な脅威カバレッジを確保できます。ただし、STRIDE のみを使用する場合は、リスク スコアリングを組み込み、現実的な攻撃シナリオをシミュレートし、脅威をビジネスおよび運用コンテキストに合わせるための追加手順が必要です。これらの機能は、PASTA の手法に本質的に統合されています。

PASTA の 7 段階プロセスでは、MITRE ATLAS の AI 固有の敵対的戦術データベース（回避、汚染、モデル抽出、推論攻撃など）を脅威マッピングに組み込むことで、脅威分析フェーズを強化します。この統合により、リスク中心モデルがビジネスの優先事項と技術的範囲と一致すると同時に、最も重要な攻撃ベクトルに対する一連の攻撃的 AI テストに直接情報を提供します。 AI 固有の敵対的戦術（例えば、回避、汚染、モデル抽出など）は、レッド チーム演習のような専門的な AI セキュリティ評価の主要な対象です。この重点は、OWASP AI Red Teaming Framework<sup>[14]</sup> に正式に規定されており、AI システムに対するこれらの攻撃ベクトルをシミュレートおよび評価する方法が定義されています。

効果的な脅威モデリングは、保護すべき重要な資産を中心に分析の範囲を定めることから始まります。そのためには、まずシステムのアーキテクチャを、必須のコンポーネント、サービス、データ ストア、インターフェース、およびサポート インフラストラクチャに分解します。次に、エンド ツー エンドで情報を追跡し、入口と出口を強調し、信頼境界を確立するデータフロー図を描くことで、これらの構成要素がどのように相互作用するかをマッピングします。データが保存、処理、送信される場所を視覚化することで、リスクにさらされている資産を正確に特定し、各コンポーネントと境界に対する潜在的な脅威と脆弱性を体系的に特定できます。この構造化されたアプローチにより、脅威モデルは焦点が絞られ、包括的であり、組織のセキュリティの優先事項と一致したものになります。

これらのスコープ設定と分解、重要な資産の特定、システムのコア コンポーネントへの分割、そしてデータフロー図を用いたエンド ツー エンドのインタラクションと信頼境界のマッピングといった活動は、STRIDE から PASTA に至るまで、多くの脅威モデリング手法に共通する基本的なステップであり、リスクの特定と優先順位付けに対する一貫性のある徹底的なアプローチを保証します。

SAIF に準拠した層（アプリケーション、データ、モデル、インフラストラクチャ）に重点を置くことで、脅威分析を意図的に高アーキテクチャ レベルに維持しています。これにより、システムのすべてのサブ コンポーネントを詳細に調査することなく、AI 固有のリスクを幅広くカバーできます。

この AI 脅威モデルでは、AI 固有の脅威を含む脅威をアプリケーション、データ、モデル、インフラストラクチャの各レイヤーにマッピングし、包括的な網羅性を確保しています。脅威の軽減策は、テスト可能な要件として定義され、検証活動は本ガイドに文書化されています。目標は、特定された脅威<sup>（注）</sup>に対する AI システムのセキュリティ態勢を評価するための包括的なテスト セットを提供することです。

注: OWASP AI Testing Guide は、デプロイメント後のセキュリティ評価を対象としており、MLOps ライフサイクル全体を網羅するものではないことにご注意ください。データ準備、モデルの学習、CI/CD パイプラインの早い段階で敵対的堅牢性テストを組み込むためのガイダンスを探しているチームには、参考文献 [16] "Securing AI/ML Systems in the Age of Information Warfare" のホワイト ペーパーをお勧めします。このホワイト ペーパーでは、AI/ML 開発プロセスにおける敵対的テスト手法について深く掘り下げた解説が提供されています。また、参考文献 [17] の John Sotiroupulos 氏の著書も確認してください。

## アーキテクチャの分解

脅威モデリングにおけるアーキテクチャ分解とは、システムを主要コンポーネント、データフロー、資産、信頼境界に分解するプロセスです。脅威が発生する可能性のある場所を特定し、体系的な脅威列挙（STRIDE など）をサポートし、攻撃対象領域を浮き彫りにして、潜在的な侵入ポイントと露出領域をすべて特定するのに役立ちます。

PASTA のステージ III に続いて、AI アーキテクチャを分解し、SAIF で定義された 4 つの層とコンポーネント グループ（データ、モデル、インフラストラクチャ、アプリケーション）に整理することで、構造化された包括的な脅威分析を可能にします。

SAIF (Secure AI Framework) モデルは、AI システムの高レベルなアーキテクチャ ビューを提供し、AI ライフサイクルのセキュリティ確保に不可欠なデータ、モデル、アプリケーション、インフラストラクチャなどの幅広いコンポーネント カテゴリを捉えるように設計されています。この抽象化は、AI セキュリティの共通ベースラインを確立する上で有用ですが、あらゆる特定の実装パターンの詳細な分解を提供することを意図したものではありません。

AI 脅威モデリングにおいて推奨されるアプローチは、Google の SAIF や OWASP AI Security Matrix といったフレームワークが提供するような、データ、モデル、アプリケーション、インフラの各層を包括的にカバーする高レベルのアーキテクチャ ビューから始まります。そこから、AI システムの具体的な導入環境（関連するテクノロジー、データフロー、統合ポイントなど）を反映する詳細レベルまでモデルを洗練させる必要があります。

このより深いレベルのモデリングは、特定の AI 使用事例に結びついた実際の攻撃対象領域を特定するために不可欠です。例えば、従業員の経費精算を自動化するロボティック・プロセス・オートメーション（RPA） ワークフローでは、[21] で説明されているように、脅威モデリングによってサードパーティー統合、データ処理、ビジネス ロジックにおけるリスクを把握する必要があります。マルチ エージェント システム（MAS）や検索拡張生成（RAG）<sup>（注）</sup>パイプラインなどのより複雑なアーキテクチャでは、脅威モデリングは SAIF 単独では提供できない範囲を拡張し、[22] で説明されているように、非常に具体的なレベルで脅威、脆弱性、および管理策を網羅する必要があります。

SAIF は、スコープ設定には役立ちますが、これらのハイブリッド化され動的にオーケストレーションされたコンポーネントを完全に分析するために必要な粒度を提供できない可能性があります。従って、脅威の状況を評価し、実際の AI デプロイメントにおける管理策の有効性をテストするには、より深い分解が必要です。

----------------------------------
<div style="font-size: small">

- 注: RAG（Retrieval-Augmented Generation）は、SAIF アーキテクチャでは明示的に定義されていませんが、その複合構造により、複数の SAIF コンポーネントにマッピングされます。RAG は、データ（汚染の影響を受けやすい外部ソース）、モデル（プロンプト操作の影響を受けやすい）、アプリケーション層（取得と生成を調整し、連鎖リスクをもたらす）、インフラストラクチャ（セキュアな構成を必要とするベクトル DB と LLM サービスをサポート）で構成されます。

</div>

## アプリケーション層

アプリケーション層は、アプリケーションと、それに関連するエージェントやプラグインを包含します。入出力に関してはユーザーと、処理と応答に関しては AI モデルとインターフェースします。エージェントとプラグインは機能を拡張しますが、管理が必要な追加の推移的リスクも生じます。

「アプリケーション」とは、AI モデルを活用して機能を提供する製品、サービス、または機能を指します。アプリケーションは、カスタマー サービス チャットボットのようにユーザー向けのものもあれば、内部システムがモデルと対話して上流プロセスをサポートするサービス指向のものもあります。

「エージェント/プラグイン」とは、AI アプリケーションまたはモデルによって特定のタスクを実行するために呼び出されるサービス、アプリケーション、または補足モデルを指します。これは「ツールの利用」と呼ばれることがよくあります。エージェントまたはプラグインは、外部データにアクセスしたり、他のモデルへのリクエストを開始したりできるため、呼び出しごとに追加の推移的リスクが生じ、AI 開発プロセスに固有の既存のリスクがさらに悪化する可能性があります。

アプリケーション層は、以下のサブ コンポーネントに分解できます。

- **ユーザー (SAIF #1)**: リクエストを発行しレスポンスを受信する人物またはシステムです。
- **ユーザー入力 (SAIF #2)**: ユーザーが送信する入力 (クエリ、コマンド) です。
- **ユーザー出力 (SAIF #3)**: アプリケーションからユーザーに返される、ユーザー アクションへの応答などの出力です。
- **アプリケーション (SAIF #4)**: ユーザー I/Oを受け取り、AI モデルを呼び出すか外部サービスを呼び出すかを決定し、レスポンスの書式を整えるコア ロジックです。
- **エージェント/プラグイン (SAIF #5) **<sup>（注）</sup>: アプリケーションやモデルと対話して特定の機能を提供するプロセスです。検索ツールや、機能を拡張する一方で追加の信頼境界を導入するサードパーティー API などです。
- **外部ソース (SAIF #6)**: これらのエージェントが依存するデータベース、サービス、または API です。それぞれが外部エンティティと潜在的なリスク ポイントを表します。

----------------------------------
<div style="font-size: small">

- 注: このガイドは、テスト目的で SAIF 定義の資産に対する脅威のマッピングに範囲を限定しています。 AI 脅威モデリングの範囲として SAIF を選択した理由は、付録 A に記載されています。
- 注: SAIF #5（エージェント/プラグイン）から SAIF #6（外部ソース）への点線は、プラグインが実行時に外部サービスから信頼できないデータを動的に取得または照会することを反映しています。

</div>

## モデル層

モデル層は、AI または ML のコア コンポーネント自体、つまり入力を出力に変換するロジック、パラメータ、実行時をカバーし、アプリケーション（およびエージェント／プラグイン）と基盤となるインフラストラクチャまたはデータの間に位置します。この層は、AI の「ブラック ボックス」を体現しているため、汚染、漏洩、または誤用を防ぐために、入力、出力、および推論操作を慎重に処理する必要があります。

モデル層は、以下のサブ コンポーネントに分解できます。

- **入力の処理 (SAIF #7)**<sup>（注）</sup>: その目的は、モデルに到達する前にすべてのデータ、プロンプト、または特徴ベクトルを検証および無害化し、インジェクション攻撃、データ汚染、または意図しない動作につながる可能性のある不正な入力を防ぐことです。入力の処理は、3 つの主要機能で構成されます。不正なデータをクリーンアップまたは拒否する入力検証機能、許可された呼び出し元のみを許可する認証と認可機能、そしてサービス拒否攻撃や総当り攻撃を防ぐレート リミッターです。
- **出力の処理 (SAIF #8)**<sup>（注）</sup>: その目的は、モデル出力をフィルタリング、編集、または後処理し、機密性の高い学習データの漏洩、プライバシーの侵害、または有害なコンテンツの生成を防ぐことです。これには、有害なコンテンツや許可されていないコンテンツを検出してブロックする出力フィルター、機密情報や個人情報を削除する無害化と編集機能、そして出力が配信前に書式の整形とビジネス ルールに準拠していることを確認する応答検証機能が含まれます。
- **モデルの利用 (SAIF #9)**: 制御された監査可能な環境において、承認された入力に対してモデルを実行し、実行時に推論ロジックが改ざんまたは破壊されないようにします。これには、重みを読み込み出力を計算する推論エンジン、ガードレールを適用するポリシー適用（トークン制限、安全なデコードなど）、および追跡可能性のために入力、モデルバージョン、および出力を記録する監査ロガーが含まれます。

----------------------------------
<div style="font-size: small">

- 注: SAIF #5（エージェント/プラグイン）から SAIF #7 (入力の処理) および SAIF #8（出力の処理）への 2 本の点線は、プラグインがプロンプトを動的に変更したり、モデル出力を後処理したりする方法を示しています。

</div>

## インフラストラクチャ層

インフラストラクチャ層は、他のすべての AI システムのコンポーネントをホストおよび接続するための基盤となるコンピューティング、ネットワーク、ストレージ、オーケストレーション サービスを提供します。リソースのプロビジョニング、分離、安全な管理を保証し、データ処理、モデル学習、推論、監視まで、あらゆるプロセスをサポートします。

インフラストラクチャ層は、以下のサブ コンポーネントに分解できます<sup>（注）</sup>。

- **モデル ストレージ インフラストラクチャ (SAIF #10)**: このコンポーネントは、重みファイル、構成データ、バージョン管理されたメタデータなどのモデル アーティファクトの保存と取得を保護し、それらの機密性、完全性、可用性を維持します。アーティファクト リポジトリはバージョン管理を維持し、保存時に暗号化を適用します。一方、完全性検証ツールは、アップロードとダウンロードのたびに暗号化ハッシュ（SHA-256 など）を計算および検証し、改ざんを検出します。鍵管理サービスは、最小権限ポリシーに基づいて暗号化鍵を発行および変更し、保存されたモデルの不正な復号を防止します。
- **モデル サービング インフラストラクチャ (SAIF #11)**: このコンポーネントは、モデルが推論リクエストを実行する実行時環境を提供します。モデル実行プロセスを他のワークロードから分離し、リソース クォータとレート制限を適用し、適切にフォーマットされた入力のみがモデルに到達するようにします。正常性監視機構は障害やパフォーマンスの低下を検出し、自動スケーリングまたは負荷分散は要求の変化下でも中断のない可用性を確保します。
- **モデルの評価 (SAIF #12)**: このコンポーネントは、デプロイ前後のモデルのパフォーマンス、公平性、堅牢性を測定します。検証スイートは、敵対的入力やエッジ ケース入力を含む予約済みのテスト セットに対してモデルを実行し、精度、バイアス、エラー率に関する指標を収集します。ドリフト検出ツールは、新しい出力を過去のベースラインと比較し、大きな逸脱をフラグ付けします。また、レポート ダッシュボードは、修正措置のための回帰やポリシー違反を明らかにします。
- **モデルの学習と調整 (SAIF #13)**: このコンポーネントは、キュレーションされたデータセット上でモデルを作成および改良するエンド ツー エンドのプロセスをオーケストレーションします。学習パイプラインは、制御された条件下でデータの前処理、特徴量エンジニアリング、反復的なモデル フィッティングを管理します。ハイパー パラメータ管理ツールは各実験の設定と結果を記録し、データ無害化ルーチンは機密情報を匿名化またはフィルタリングすることで、学習中のプライバシーを保護します。
- **モデル フレームワークとコード (SAIF #14)**: このコンポーネントには、モデル アーキテクチャと学習ルーチンを定義するライブラリ、フレームワーク、カスタム コードが含まれます。静的解析および依存関係スキャン ツールは、サードパーティー製パッケージの既知の脆弱性を検出します。設計段階からセキュリティを重視したコード レビューにより、安全でない動的実行やハード コードされた認証情報の回避といったベスト プラクティスが徹底され、強化された実行時環境により、モデル サービング コードや学習コードの攻撃対象領域が制限されます。
- **データ ストレージ インフラストラクチャ (SAIF #15)**: 「データ」は SAIF 独自のドメインにまたがっていますが、特徴量ストアや埋め込みインデックスといったモデル固有のストレージ システムには、専用のセキュリティ管理策が必要です。これらのストアは、アクセス ポリシーを適用し、データ スキーマと書式を検証し、すべての読み取り/書き込み操作をログに記録して追跡可能性を確保します。保存時および転送中の暗号化により、機密性の高い入力データと中間アーティファクトを保護し、定期的な完全性チェックにより不正な変更が行われないようにします。

## データ層

データ層は、モデルが利用する生データと処理済みデータを提供することで、あらゆる AI システムの基盤となります。データ層は、初期の収集と取り込みから、変換、保存、そして学習や推論のためのプロビジョニングに至るまで、データのライフサイクル全体を網羅し、データの正確性と信頼性を維持し、プライバシーとセキュリティ ポリシーへの準拠を保証します。この層の堅牢な管理策により、データの汚染、漏洩、不正アクセスから保護され、信頼性が高く責任ある AI 成果の基盤を形成します。

データ層は、以下のサブ コンポーネントに分解できます。

- **学習データ (SAIF #16）**: 学習データは、モデルにパターン認識と予測方法を学習させるために使用される、キュレーションされたラベル付きサンプル データで構成されます。セキュアな AI パイプラインでは、組織は学習データセットの完全性を保証するために、厳格な出所とバージョン管理を確立します。すべてのレコードの出所、変更履歴、アクセス イベントはログに記録され、監査可能な状態にします。学習リポジトリに対して保存時の暗号化とロール ベースの権限設定を適用することで、システムは不正な改ざんを防止します。学習コーパスへの不正な変更は、モデルの学習プロセスを破壊し、敵対的な操作につながる可能性があります。
- **データのフィルタリングと処理 (SAIF #17）**：生の入力データをモデル パイプラインに取り込む前に、データは厳格なフィルタリングと処理のステップを経ます。これには、スキーマ検証、破損または悪意のあるエントリを除去するための異常検出、匿名化や仮名化などのプライバシー保護のための変換が含まれます。セキュアな処理フレームワークは、適用されたすべての変換を記録する再現可能なパイプラインを使用して、これらのタスクを隔離された環境で実行します。各段階できめ細かなアクセス制御と変更追跡を組み込むことで、システムは検証済みで無害化されたデータのみがモデルに影響を与えることを保証し、偶発的なエラーと意図的なデータ汚染攻撃の両方によるリスクを軽減します。
- **データ ソース (SAIF #18)**<sup>（注）</sup>: AI システムのデータは、社内運用データベース、ユーザー生成の入力、IoT センサー、またはサードパーティー プロバイダーから取得される場合があります。内部ソースは組織のポリシーによって管理され、アクセス異常が監視されます。
- **外部データソース (SAIF #19)**: これらのソースには、購入した市場データやパブリック API など、品質、ライセンス コンプライアンス、セキュリティに関する追加審査が必要な外部データ フィードが含まれる場合があります。組織は、これらの外部接続を保護するために、契約および技術上の管理（暗号化チャネル、相互認証など）を実施し、フィードの健全性と完全性を継続的に監査します。

----------------------------------
<div style="font-size: small">

- 注: SAIF アーキテクチャにおける SAIF #4（アプリケーション）から SAIF #18（内部データソース）への点線矢印はフィードバック ループを表しています。このループでは、ユーザー入力、インタラクション ログ、モデル出力など、アプリケーション実行時に生成されるデータが内部的に捕捉され、保存されます。このデータは、後でモデルの微調整や再学習に使用できます。

</div>

# 2.1. AI 脅威の特定

## 事業影響度分析 (BIA)

## 情報セキュリティ リスクに対する CIA ベースの脅威分析

## CIA 脅威の AI アーキテクチャ層および AI コンポーネントへのマッピング

## 2.1.1. OWASP 脅威のアーキテクチャマッピング

## 2.1.2. AI システムの RAI 脅威の特定

### 責任ある AI のみ

### AI アプリケーションの RAI 脅威

### AI モデルの RAI 脅威

### AI インフラストラクチャの RAI 脅威

### AI データの RAI 脅威

### AI テストの 4 つの柱

#### 1. AI アプリケーションのテスト

#### 2. AI モデルのテスト

#### 3. AI インフラストラクチャのテスト

#### 4. AI データのテスト
